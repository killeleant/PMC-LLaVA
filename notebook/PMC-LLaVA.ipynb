{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yZquKRCZw5k"
   },
   "outputs": [],
   "source": [
    "#    Copyright 2023 Haotian Liu\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgLe6WCNzO2l"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZBcNAMWU9DM"
   },
   "source": [
    "## 1.0 Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJTwQVfZU4Ul"
   },
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip -q\n",
    "!pip install matplotlib -q -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcFUmZIMVK12"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets\n",
    "!pip install transformers -q -U\n",
    "!pip install -q bitsandbytes sentencepiece accelerate loralab\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install hf_transfer -q -U\n",
    "!pip install pickleshare -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hq4qOYOvWDyf"
   },
   "outputs": [],
   "source": [
    "#%env HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "if not os.path.exists(\"./CS45_2_S1_2024\"): # modify the path\n",
    "    !git clone https://github.com/theon1130/CS45_2_S1_2024.git\n",
    "else:\n",
    "    print(\"CS45_2_S1_2024 already exists\")"
   ],
   "metadata": {
    "id": "DQsqSLicPjg5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHCWKGIQWYKH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"/workspace/LLaVA\"):\n",
    "    !git clone https://github.com/haotian-liu/LLaVA.git\n",
    "else:\n",
    "    print(\"LLaVA already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replace the llava file in LLaVA with our llava in CS45_2_S1_2024"
   ],
   "metadata": {
    "id": "zaRdbpaYbNeg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUwyhPOQjDWs"
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "%cd ./LLaVA\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBUU7nmEjGdd"
   },
   "outputs": [],
   "source": [
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qP3ko3n-kvZx"
   },
   "outputs": [],
   "source": [
    "!pip install protobuf -q -U\n",
    "!pip install --upgrade Pillow -q\n",
    "!pip install -e \".[train]\" -q\n",
    "!pip install flash-attn --no-build-isolation -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPwAw4RHmg5A"
   },
   "source": [
    "## 2.0 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j640Z63iYcSA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import AutoProcessor, Trainer, BitsAndBytesConfig, TrainingArguments, AutoTokenizer\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sH2E3EYbOhv"
   },
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.utils import disable_torch_init\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"your path to the downloaded model from hugging face\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8MQQITeb81Q"
   },
   "outputs": [],
   "source": [
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path = model_path,\n",
    "    model_base = None,\n",
    "    model_name = model_name,\n",
    "    #use_flash_attn=True,\n",
    "#    cache_dir = ''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "load lora for VQARADqformer"
   ],
   "metadata": {
    "id": "yOw9J8b3sIB3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "peft_model_id = \"your path\"\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()"
   ],
   "metadata": {
    "id": "YvUtYYwasHFn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "load lora for SLAKEqformer"
   ],
   "metadata": {
    "id": "vtpUPNgZseqp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "peft_model_id = \"your path\"\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()"
   ],
   "metadata": {
    "id": "Dh3vcSW6sWu_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bncg2DjGdqyE"
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "print('='*100)\n",
    "print(image_processor)\n",
    "print('='*100)\n",
    "print(tokenizer)\n",
    "print('='*100)\n",
    "print(context_len)\n",
    "print(tokenizer.model_max_length)\n",
    "print('='*100)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO7yl_1kgJ9J"
   },
   "source": [
    "## 3.0 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgg_D5yKgOOb"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    "    IGNORE_INDEX,\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    ")\n",
    "\n",
    "def creat_prompt(qs, model, model_name=model_name, caption=None):\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in qs:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "        else:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "    else:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            qs = image_token_se + \"\\n\" + qs\n",
    "        else:\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "\n",
    "\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    if caption:\n",
    "        conv.append_message(conv.roles[1], caption)\n",
    "    else:\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLukleKHmBCh"
   },
   "outputs": [],
   "source": [
    "def image_parser(args):\n",
    "    out = args.image_file.split(args.sep)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if isinstance(image_file, str):\n",
    "      if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "          response = requests.get(image_file)\n",
    "          image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "      else:\n",
    "          image = Image.open(image_file).convert(\"RGB\")\n",
    "    elif isinstance(image_file, Image.Image):\n",
    "        image = image_file\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image file type: {type(image_file)}\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ld0VNSatnM0j"
   },
   "outputs": [],
   "source": [
    "def process_and_prepare_image(image_files, model, image_processor, device):\n",
    "    images = load_images(image_files)\n",
    "    images_tensor = process_images(images, image_processor, model.config)\n",
    "\n",
    "    images_tensor_to_device = [image_tensor.to(device, dtype=torch.bfloat16) for image_tensor in images_tensor]\n",
    "\n",
    "    image_sizes = [image.size for image in images]\n",
    "    return images_tensor_to_device, image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psdy3Beqme1-"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPVisionConfig,\n",
    "    CLIPVisionModel,\n",
    "    InstructBlipQFormerConfig,\n",
    "    InstructBlipQFormerModel,\n",
    "    InstructBlipProcessor,\n",
    ")\n",
    "\n",
    "QformerProcessor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xxl\")\n",
    "\n",
    "def preprocess_text(strs):\n",
    "    tokenized_text = QformerProcessor(text=strs, padding=True, return_tensors=\"pt\")\n",
    "    qformer_ids = tokenized_text[\"qformer_input_ids\"].to(model.device)\n",
    "    attention_mask = tokenized_text[\"qformer_attention_mask\"].to(model.device)\n",
    "    return qformer_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_SBdt2ImR5C"
   },
   "outputs": [],
   "source": [
    "def eval_model(tokenizer, model, image_processor, context_len, image_files, qs, use_q,  sep=',', model_name=model_name, temperature=1.0, num_beams=1, max_new_tokens=512):\n",
    "    disable_torch_init()\n",
    "\n",
    "    qformer_ids, qformerattention_mask = preprocess_text(qs)\n",
    "\n",
    "    prompt = creat_prompt(qs, model, model_name)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "\n",
    "    images_tensor, image_sizes = process_and_prepare_image(image_files, model, image_processor, model.device) # image_files should be a str list\n",
    "\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .to(model.device)\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            qformer_ids,\n",
    "            qformerattention_mask,\n",
    "            images=images_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            use_q=use_q,\n",
    "            do_sample=True if temperature != 1.0 else False,\n",
    "            temperature=temperature,\n",
    "            #top_p=top_p,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(outputs)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyEpkm4KqXxB"
   },
   "source": [
    "## 4.0 Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbFyNUiyIm0u"
   },
   "source": [
    "### 4.1 Prepare for ROCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSemgRB3ItC6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('/train/radiologytraindata.csv')\n",
    "train_prefix = \"Your path\"\n",
    "train_df[\"name\"] = train_df[\"name\"].apply(lambda x: train_prefix + x)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m26bwSrUXDYE"
   },
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('/validation/radiologytraindata.csv')\n",
    "val_prefix = \"Your paht\"\n",
    "val_df[\"name\"] = val_df[\"name\"].apply(lambda x: val_prefix + x)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhyX7v6pN4hR"
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)\n",
    "eval_ds = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHBhltV03GE-"
   },
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i_6Ef6M15K2"
   },
   "outputs": [],
   "source": [
    "print(eval_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNPOxtGY2e67"
   },
   "source": [
    "prepare the dataset that will be used for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDveCTskuaH9"
   },
   "outputs": [],
   "source": [
    "concise_describe_instructions = [\n",
    "    \"Describe the image concisely.\",\n",
    "    \"Provide a brief description of the given image.\",\n",
    "    \"Offer a succinct explanation of the picture presented.\",\n",
    "    \"Summarize the visual content of the image.\",\n",
    "    \"Give a short and clear explanation of the given image.\",\n",
    "    \"Share a concise interpretation of the image provided.\",\n",
    "    \"Present a compact description of the photo's key features.\",\n",
    "    \"Relay a brief, clear account of the picture shown.\",\n",
    "    \"Render a clear and concise summary of the photo.\",\n",
    "    \"Write a terse but informative summary of the provided picture.\",\n",
    "    \"Briefly describe this image.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTeguuvGH7dp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize_and_create_label(example_batch, image_processor, tokenizer, model, model_name, device):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    image_files = example_batch[\"name\"]\n",
    "\n",
    "    images_tensor, image_sizes = process_and_prepare_image(image_files, model, image_processor, model.device)\n",
    "\n",
    "    tokenized_conversation_with_caption = []\n",
    "    tokenized_conversation_without_caption = []\n",
    "    query_list = []\n",
    "    for caption in example_batch[\"caption\"]:\n",
    "        query = random.choice(concise_describe_instructions)\n",
    "        query_list.append(query)\n",
    "        prompt_without_caption = creat_prompt(query, model, model_name, None)\n",
    "        prompt_with_caption = creat_prompt(query, model, model_name, caption)\n",
    "\n",
    "        tokenized_without_caption = tokenizer_image_token(prompt_without_caption, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        tokenized_with_caption = tokenizer_image_token(prompt_with_caption, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "\n",
    "        tokenized_conversation_without_caption.append(tokenized_without_caption)\n",
    "        tokenized_conversation_with_caption.append(tokenized_with_caption)\n",
    "\n",
    "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversation_with_caption], batch_first=True, padding_value=pad_token_id)\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "\n",
    "    labels = torch.full_like(input_ids, fill_value=IGNORE_INDEX)\n",
    "    for i, tcwc in enumerate(tokenized_conversation_without_caption):\n",
    "        input_id_without_caption = tcwc.squeeze(0)\n",
    "        labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption):]\n",
    "\n",
    "    qformer_ids_list, qformerattention_list = preprocess_text(query_list)\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"qformer_inputids\": qformer_ids_list,\n",
    "        \"qfromer_attention_mask\": qformerattention_list,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"images\": images_tensor,\n",
    "        \"image_sizes\": image_sizes,\n",
    "    }\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def transform_batch(batch):\n",
    "    return tokenize_and_create_label(batch, image_processor, tokenizer, model, model_name, device)\n",
    "\n",
    "\n",
    "train_ds.set_transform(transform_batch)\n",
    "eval_ds.set_transform(transform_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VgTpi04o_SU"
   },
   "source": [
    "### 4.2 Prepare for VQA-RAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m_Iwe3e4ym-"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "cache_dir = \"./VQA-RAD\"\n",
    "VQA_RADdataset = load_dataset(\"flaviagiammarino/vqa-rad\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlISnr1G46eU"
   },
   "outputs": [],
   "source": [
    "print(VQA_RADdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xKtX5hLqJfl"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "path = \"./VQA_RAD Dataset Public.json\"\n",
    "#path = \"/workspace/VQA_RAD_Dataset_Public.json\"\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    vqa_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byitnY8ZtZjX"
   },
   "outputs": [],
   "source": [
    "test_data = VQA_RADdataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjUhiaMw1YV_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r'\\s+', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "def add_image_name_and_answer_type(example):\n",
    "    question = normalize_text(example['question'])\n",
    "    answer = normalize_text(example['answer'])\n",
    "    for item in vqa_data:\n",
    "        if normalize_text(item['question']) == question and normalize_text(item['answer']) == answer:\n",
    "            example['image_name'] = item['image_name']\n",
    "            example['answer_type'] = item['answer_type']\n",
    "            return example\n",
    "\n",
    "    raise ValueError(f\"No matching question-answer pair found for question: {example['question']}, answer: {example['answer']}\")\n",
    "\n",
    "train_datas = VQA_RADdataset['train'].map(add_image_name_and_answer_type)\n",
    "train_ds, eval_ds = train_datas.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-xWwYhKvy8J"
   },
   "outputs": [],
   "source": [
    "modified_test_data = test_data.map(add_image_name_and_answer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuxsJmr4k3KA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pre_prompt = {\n",
    "    \"short\": \"Based on the image, respond to this question with a word or phrase: \",\n",
    "    \"long\": \"Based on the image, respond to this question with a short answer: \"\n",
    "}\n",
    "\n",
    "def prepare_data(example):\n",
    "\n",
    "    if \"CLOSED\" in example[\"answer_type\"]:\n",
    "        example[\"question\"] = pre_prompt[\"short\"] + example[\"question\"]\n",
    "    elif example[\"answer_type\"] == \"OPEN\":\n",
    "        example[\"question\"] = pre_prompt[\"long\"] + example[\"question\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid answer type: {example['answer_type']}\")\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(prepare_data)\n",
    "eval_ds = eval_ds.map(prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQLBmIJLmyK5"
   },
   "outputs": [],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BiNTuQB8m21L"
   },
   "outputs": [],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IS8uyEGpWRg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize_and_create_label(example_batch, image_processor, tokenizer, model, model_name, device):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    image_files = example_batch[\"image\"]\n",
    "\n",
    "    images_tensor, image_sizes = process_and_prepare_image(image_files, model, image_processor, model.device)\n",
    "\n",
    "    tokenized_conversation_with_caption = []\n",
    "    tokenized_conversation_without_caption = []\n",
    "    query_list = []\n",
    "    for query, answer in zip(example_batch[\"question\"], example_batch[\"answer\"]):\n",
    "\n",
    "        query_list.append(query)\n",
    "        prompt_without_caption = creat_prompt(query, model, model_name, None)\n",
    "        prompt_with_caption = creat_prompt(query, model, model_name, answer)\n",
    "\n",
    "        tokenized_without_caption = tokenizer_image_token(prompt_without_caption, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        tokenized_with_caption = tokenizer_image_token(prompt_with_caption, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "\n",
    "        tokenized_conversation_without_caption.append(tokenized_without_caption)\n",
    "        tokenized_conversation_with_caption.append(tokenized_with_caption)\n",
    "\n",
    "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversation_with_caption], batch_first=True, padding_value=pad_token_id)\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "\n",
    "    labels = torch.full_like(input_ids, fill_value=IGNORE_INDEX)\n",
    "    for i, tcwc in enumerate(tokenized_conversation_without_caption):\n",
    "        input_id_without_caption = tcwc.squeeze(0)\n",
    "        labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption):]\n",
    "\n",
    "    qformer_ids_list, qformerattention_list = preprocess_text(query_list)\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"qformer_inputids\": qformer_ids_list,\n",
    "        \"qfromer_attention_mask\": qformerattention_list,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"images\": images_tensor,\n",
    "        \"image_sizes\": image_sizes,\n",
    "    }\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def transform_batch(batch):\n",
    "    return tokenize_and_create_label(batch, image_processor, tokenizer, model, model_name, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEIk9kvY1y_a"
   },
   "outputs": [],
   "source": [
    "train_ds.set_transform(transform_batch)\n",
    "eval_ds.set_transform(transform_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGomMIzdpCv-"
   },
   "source": [
    "### 4.3 Prepare for SLAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIKS87FSuVW_"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cache_dir = \"./SLAKE\"\n",
    "slake_dataset = load_dataset(\"BoKelvin/SLAKE\", cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhdiVgo4uVXA"
   },
   "outputs": [],
   "source": [
    "print(slake_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDjnEurbSRh5"
   },
   "outputs": [],
   "source": [
    "train_ds = slake_dataset['train'].filter(lambda example: example['q_lang'] == 'en')\n",
    "eval_ds = slake_dataset['validation'].filter(lambda example: example['q_lang'] == 'en')\n",
    "test_ds = slake_dataset['test'].filter(lambda example: example['q_lang'] == 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ch1DlJkGef4E"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path_pre = \"./Slake1.0/imgs/\"\n",
    "\n",
    "pre_prompt = {\n",
    "    \"short\": \"Based on the image, respond to this question with a word or phrase: \",\n",
    "    \"long\": \"Based on the image, respond to this question with a short answer: \"\n",
    "}\n",
    "\n",
    "def prepare_data(example):\n",
    "    example['img_name'] = os.path.join(path_pre, example['img_name'])\n",
    "\n",
    "    if example[\"answer_type\"] == \"CLOSED\":\n",
    "        example[\"question\"] = pre_prompt[\"short\"] + example[\"question\"]\n",
    "    elif example[\"answer_type\"] == \"OPEN\":\n",
    "        example[\"question\"] = pre_prompt[\"long\"] + example[\"question\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid answer type: {example['answer_type']}\")\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "train_ds = train_ds.map(prepare_data)\n",
    "eval_ds = eval_ds.map(prepare_data)\n",
    "test_ds = test_ds.map(prepare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dE90vxQpwiMU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tokenize_and_create_label(example_batch, image_processor, tokenizer, model, model_name, device):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    image_files = example_batch[\"img_name\"]\n",
    "\n",
    "    images_tensor, image_sizes = process_and_prepare_image(image_files, model, image_processor, model.device)\n",
    "\n",
    "    tokenized_conversation_with_caption = []\n",
    "    tokenized_conversation_without_caption = []\n",
    "    query_list = []\n",
    "    for query, answer in zip(example_batch[\"question\"], example_batch[\"answer\"]):\n",
    "\n",
    "        query_list.append(query)\n",
    "        prompt_without_caption = creat_prompt(query, model, model_name, None)\n",
    "        prompt_with_caption = creat_prompt(query, model, model_name, answer)\n",
    "\n",
    "        tokenized_without_caption = tokenizer_image_token(prompt_without_caption, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        tokenized_with_caption = tokenizer_image_token(prompt_with_caption, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "\n",
    "        tokenized_conversation_without_caption.append(tokenized_without_caption)\n",
    "        tokenized_conversation_with_caption.append(tokenized_with_caption)\n",
    "\n",
    "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversation_with_caption], batch_first=True, padding_value=pad_token_id)\n",
    "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
    "\n",
    "    labels = torch.full_like(input_ids, fill_value=IGNORE_INDEX)\n",
    "    for i, tcwc in enumerate(tokenized_conversation_without_caption):\n",
    "        input_id_without_caption = tcwc.squeeze(0)\n",
    "        labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption):]\n",
    "\n",
    "    qformer_ids_list, qformerattention_list = preprocess_text(query_list)\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"qformer_inputids\": qformer_ids_list,\n",
    "        \"qfromer_attention_mask\": qformerattention_list,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "        \"images\": images_tensor,\n",
    "        \"image_sizes\": image_sizes,\n",
    "    }\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def transform_batch(batch):\n",
    "    return tokenize_and_create_label(batch, image_processor, tokenizer, model, model_name, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAGRLbGJkn9D"
   },
   "outputs": [],
   "source": [
    "train_ds.set_transform(transform_batch)\n",
    "eval_ds.set_transform(transform_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZDMtVx-kVll"
   },
   "source": [
    "## 5.0 Evaluation on benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DxjF3PrvIBL"
   },
   "source": [
    "### 5.1 Eval VQA-RAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XSnYCTI5nw_"
   },
   "outputs": [],
   "source": [
    "print(modified_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4h8YLPvI4pWr"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "json_data = []\n",
    "\n",
    "for row in modified_test_data:\n",
    "\n",
    "    image = row['image']\n",
    "    answer = row['answer']\n",
    "\n",
    "    if row['answer_type'] == 'OPEN':\n",
    "        qs = f\"Based on the image, respond to this question with a short answer:{row['question']}\"\n",
    "    elif row['answer_type'] == 'CLOSED':\n",
    "        qs = f\"Based on the image, respond to this question with a word or phrase:{row['question']}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid answer_type: {row['answer_type']}\")\n",
    "\n",
    "    generate_answer = eval_model(tokenizer, model, image_processor, context_len, [image], qs, use_q=True)\n",
    "\n",
    "    print(f\"{counter}.Img: {image}\\n Question: {row['question']}\\n Answer: {generate_answer}\\n GT: {answer}\")\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    new_json_data = {\n",
    "        \"image_name\": row['image_name'],\n",
    "        \"question\": row['question'],\n",
    "        \"prompt\":qs,\n",
    "        \"generated\": generate_answer,\n",
    "        \"answer\": answer,\n",
    "        \"mode\": \"test\",\n",
    "        \"answer_type\": row['answer_type']\n",
    "    }\n",
    "    json_data.append(new_json_data)\n",
    "\n",
    "\n",
    "with open(\"./vqa_prediction_answer.json\", \"w\") as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Sam3wRAvKLq"
   },
   "source": [
    "### 5.2 Eval SLAKE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWM04i3sM4Ob"
   },
   "source": [
    "eval test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liUvp921M03L"
   },
   "outputs": [],
   "source": [
    "path_pre = \"./Slake1.0/imgs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3I1OY16MhMJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "counter = 0\n",
    "json_data = []\n",
    "\n",
    "for row in test_ds:\n",
    "\n",
    "    image_path = os.path.join(path_pre, row['img_name'])\n",
    "    answer = row['answer']\n",
    "\n",
    "    if row['answer_type'] == 'OPEN':\n",
    "        qs = f\"Based on the image, respond to this question with a short answer:{row['question']}\"\n",
    "    elif row['answer_type'] == 'CLOSED':\n",
    "        qs = f\"Based on the image, respond to this question with a word or phrase:{row['question']}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid answer_type: {row['answer_type']}\")\n",
    "\n",
    "    generate_answer = eval_model(tokenizer, model, image_processor, context_len, [image_path], qs, use_q=True)\n",
    "\n",
    "    print(f\"{counter}.Img: {image_path}\\n Question: {row['question']}\\n Answer: {generate_answer}\\n GT: {answer}\")\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    new_json_data = {\n",
    "        \"image_name\": row['img_name'],\n",
    "        \"question\": row['question'],\n",
    "        \"prompt\":qs,\n",
    "        \"generated\": generate_answer,\n",
    "        \"answer\": answer,\n",
    "        \"mode\": \"test\",\n",
    "        \"answer_type\": row['answer_type'],\n",
    "        'img_id' : row['img_id'],\n",
    "        'qid' : row['qid']\n",
    "    }\n",
    "    json_data.append(new_json_data)\n",
    "\n",
    "\n",
    "with open(\",/slake_prediction_answer.json\", \"w\") as json_file:\n",
    "    json.dump(json_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4zl7EZPrW_S"
   },
   "source": [
    "## 6.0 LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L68hqlMFr--"
   },
   "source": [
    "### 6.1 lora for VQA-RAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2_Zc3ozTY_P"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\",\n",
    "        \"up_proj\", \"down_proj\", \"gate_proj\",\n",
    "    ],\n",
    "    modules_to_save=[\"mm_projector\", \"query_tokens\", \"post_projection\", \"projection\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdOSqFFeTtFM"
   },
   "outputs": [],
   "source": [
    "model.base_model.model.model.vision_tower.query_tokens.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8qYFqguuoar"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyZk9lKjs71v"
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr463JKOFwbb"
   },
   "source": [
    "### 6.2 lora for SLAKE"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(model)"
   ],
   "metadata": {
    "id": "genvMcyXUxNO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2iR7tR-T-cI"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\",\n",
    "        \"up_proj\", \"down_proj\", \"gate_proj\",\n",
    "    ],\n",
    "    modules_to_save=[\"mm_projector\", \"query_tokens\", \"post_projection\", \"projection\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zX822ebXUFec"
   },
   "outputs": [],
   "source": [
    "model.base_model.model.model.vision_tower.query_tokens.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MU6ILC_hoWyl"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew-a93WqoWym"
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60kaT9jw4m6G"
   },
   "source": [
    "## 7.0 Training with hugging face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBtcAdlWEFDL"
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qJhoUs3ENnR"
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YuOt2T3Ffo7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"your key\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"your project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmkE-sMbMmF-"
   },
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NKL5CsAuVXI"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LY6nKHleuVXI"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmvX5k1xvpJk"
   },
   "source": [
    "### 7.1 Training proector with ROCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7pLB96i1teO"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.model.vision_tower.projection.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.model.vision_tower.query_tokens.requires_grad = True\n",
    "\n",
    "for param in model.model.vision_tower.post_projection.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.model.mm_projector.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4T94koYIuVXI"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=4e-5, foreach=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "263KOCg244bG"
   },
   "outputs": [],
   "source": [
    "output_model_name = f\"ROCO_{model_name}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"/workspace/checkpoints/\" + output_model_name,\n",
    "    learning_rate=4e-5,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    optimizers=(optimizer, None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfq7DQAyuVXI"
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Zd22-Rp9hN1"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOSmLBcXuVXI"
   },
   "outputs": [],
   "source": [
    "#trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGOVeFLIuVXJ"
   },
   "outputs": [],
   "source": [
    "new_model_dir = './ROCOQformer/'\n",
    "trainer.save_model(new_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fafzOXJ-Up0M"
   },
   "outputs": [],
   "source": [
    "querytoken = model.model.vision_tower.query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bf1ofE7FVGkw"
   },
   "outputs": [],
   "source": [
    "print(type(querytoken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTkjBr_rU0ml"
   },
   "outputs": [],
   "source": [
    "torch.save(querytoken, './ROCOquery_tokens.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZ5mO2_6I3s2"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM3WFhUON-Xo"
   },
   "source": [
    "### 7.2 Finetune with VQA-RAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYv3mv7dR9en"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, foreach=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0msI5yHR9eo"
   },
   "outputs": [],
   "source": [
    "output_model_name = f\"vqarad{model_name}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"/workspace/checkpoints/\" + output_model_name,\n",
    "    learning_rate=2e-5,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_steps=25,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    optimizers=(optimizer, None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC0o0MLuR9eo"
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfCsodAkiKdR"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9iF9xdoR9eo"
   },
   "outputs": [],
   "source": [
    "new_model_dir = './VQARADQformer/'\n",
    "trainer.save_model(new_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hjIYqo5Vr1P"
   },
   "outputs": [],
   "source": [
    "querytoken = model.base_model.model.model.vision_tower.query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRKayvGsVr1X"
   },
   "outputs": [],
   "source": [
    "print(type(querytoken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRzdnAE9pWRm"
   },
   "outputs": [],
   "source": [
    "print(querytoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWmaambrVr1X"
   },
   "outputs": [],
   "source": [
    "torch.save(querytoken, '.VQARADquery_tokens.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzvOtC7UR9ep"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowHNOJxOEIi"
   },
   "source": [
    "### 7.3 Finetune with SLAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrYnNNgDophh"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, foreach=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYk6Wtxuophh"
   },
   "outputs": [],
   "source": [
    "output_model_name = f\"SLAKE{model_name}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"/workspace/checkpoints/\" + output_model_name,\n",
    "    learning_rate=2e-5,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    dataloader_pin_memory=False,\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    save_steps=30,\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    optimizers=(optimizer, None)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o2g-qZoophh"
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0Kn2T2vqYxA"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd5C3NIsophi"
   },
   "outputs": [],
   "source": [
    "new_model_dir = './SLAKEQformer/'\n",
    "trainer.save_model(new_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqi8rfVmWN8N"
   },
   "outputs": [],
   "source": [
    "querytoken = model.base_model.model.model.vision_tower.query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c17sev2uWN8V"
   },
   "outputs": [],
   "source": [
    "print(type(querytoken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdykwrG2pWRn"
   },
   "outputs": [],
   "source": [
    "print(querytoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bddGy8nSWN8V"
   },
   "outputs": [],
   "source": [
    "torch.save(querytoken, './SLAKEquery_tokens.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlE41_7Cophi"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
